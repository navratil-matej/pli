{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d244b9ce",
   "metadata": {},
   "source": [
    "# Třetí cvičení - jazykové modely, perplexita\n",
    "\n",
    "## 1) Příprava dat\n",
    "\n",
    "- Soubory TEXTCZ1 a TEXTEN1 přeuspořádejte tak, aby každá věta byla na jednom řádku (jako oddělovač vět použijte tečku, otazník a vykřičník).\n",
    "- Všechny slova převeďte na lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2294128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "from collections import Counter\n",
    "from math import log\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8ef85",
   "metadata": {},
   "source": [
    "## Unigramový model\n",
    "- Vypočítejte pravděpodobnost každého ze slov\n",
    "- Vypočítejte entropii H(X) a perplexitu G(X) modelu\n",
    "\n",
    "$H(X) = - \\sum_{x}p(x)log{_2}p(x)$\n",
    "\n",
    "$G(X) = 2^{H(X)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9263d37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 2.321928094887362 Perplexity: 4.999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Nejprve vytvořte unigramový model pro větu: \"<s> Dnes je hezký den\"\n",
    "sent = \"<s> Dnes je hezký den\"\n",
    "freq = Counter(sent.split())\n",
    "total = freq.total()\n",
    "entropy = -sum(\n",
    "    (count / total) * log(count / total, 2)\n",
    "    for _, count in freq.items()\n",
    ")\n",
    "perplexity = 2 ** entropy\n",
    "\n",
    "# Očekávaný výsledek: \n",
    "# entropy = 2.321928094887362\n",
    "# perplexity = 4.999999999999999\n",
    "\n",
    "print(f\"Entropy: {entropy} Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11fd7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nyní pro CZ a EN korpusy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91ff11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Načtení souborů CZ, EN\n",
    "with open('TEXTCZ1.txt', \"rb\") as f:\n",
    "    cz_text = f.read().decode('cp1250').splitlines()\n",
    "with open('TEXTEN1.txt', \"r\") as f:\n",
    "    en_text = f.read().splitlines()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea1821ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozdělení textů na věty, odstranění interpunkce a přidání znaku začátku věty\n",
    "\n",
    "def sentences_unigram(text):\n",
    "    split_on = set('.?!')\n",
    "    sentences = [['<s>']]\n",
    "    for word in text:\n",
    "        if re.match(r'[\\w\\-]+', word):\n",
    "            sentences[-1].append(word.lower())\n",
    "        elif word in split_on:\n",
    "            sentences.append(['<s>'])\n",
    "    return sentences\n",
    "\n",
    "def unigram(sentences):\n",
    "    return Counter([\n",
    "        word\n",
    "        for sent in sentences\n",
    "        for word in sent\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d8c4255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 9.15595455066942 Perplexity: 570.4491780329872\n"
     ]
    }
   ],
   "source": [
    "# Anglický unigramový model\n",
    "\n",
    "en_sent = sentences_unigram(en_text)\n",
    "en_model = unigram(en_sent)\n",
    "en_total = en_model.total()\n",
    "\n",
    "en_entropy = -sum(\n",
    "    (count / en_total) * log(count / en_total, 2)\n",
    "    for _, count in en_model.items()\n",
    ")\n",
    "en_perplexity = 2 ** en_entropy\n",
    "\n",
    "print(f\"Entropy: {en_entropy} Perplexity: {en_perplexity}\")\n",
    "# Očekávaný výsledek: \n",
    "# en_entropy = 9.153156076444484\n",
    "# en_perplexity = 569.3437191681389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78260093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 11.834792150303567 Perplexity: 3652.8129153021596\n"
     ]
    }
   ],
   "source": [
    "# Český unigramový model\n",
    "\n",
    "cz_sent = sentences_unigram(cz_text)\n",
    "cz_model = unigram(cz_sent)\n",
    "cz_total = cz_model.total()\n",
    "\n",
    "cz_entropy =  -sum(\n",
    "    (count / cz_total) * log(count / cz_total, 2)\n",
    "    for _, count in cz_model.items()\n",
    ")\n",
    "cz_perplexity = 2 ** cz_entropy\n",
    "\n",
    "print(f\"Entropy: {cz_entropy} Perplexity: {cz_perplexity}\")\n",
    "# Očekávaný výsledek: \n",
    "# en_entropy = 11.866640196670001\n",
    "# en_perplexity = 3734.346796191333\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a4ef1",
   "metadata": {},
   "source": [
    "## Bigramový model\n",
    "- obdobně vytvořte bigramový model a všechny dvojice slov hledejte vždy pouze ve větě (nikoliv mezi větami)\n",
    "- vypočítejte entropii a perplexitu\n",
    "\n",
    "$H(B|A) = - \\sum_{a \\in A, b \\in B}P(a,b)log{_2}P(b|a)$ \n",
    "\n",
    "$G(X) = 2^{H(B|A)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59e5a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 0.4 Perplexity: 1.3195079107728942\n"
     ]
    }
   ],
   "source": [
    "# Nejprve vytvořte bigramový model pro větu: \"<s> Dnes je hezký den </s>\"\n",
    "sent = \"<s> dnes respektive dnes možná </s>\".split()\n",
    "w_freq = Counter(sent)\n",
    "b_freq = Counter(zip(sent[:-1], sent[1:]))\n",
    "total = b_freq.total()\n",
    "\n",
    "entropy = -sum( # If a pair isn't found, its contribution is times zero\n",
    "    (count / total) * log(count / w_freq[w1], 2)\n",
    "    for (w1, _), count in b_freq.items()\n",
    ")\n",
    "perplexity = 2 ** entropy\n",
    "\n",
    "print(f\"Entropy: {entropy} Perplexity: {perplexity}\")\n",
    "# Očekávaný výsledek: \n",
    "# entropy = 0.4\n",
    "# perplexity = 1.3195079107728942"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99a3dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nyní pro jednotlivé korpusy, nejprve pro CZ a pak pro EN\n",
    "\n",
    "# Přidání koncového znaku </s> do vět\n",
    "\n",
    "def sentences_bigram(text):\n",
    "    return [\n",
    "        sent + ['</s>']\n",
    "        for sent in sentences_unigram(text)\n",
    "    ]\n",
    "\n",
    "def bigram(sentences):\n",
    "    b_freq = Counter([\n",
    "        (w1, w2)\n",
    "        for sent in sentences\n",
    "        for (w1, w2) in zip(sent[:-1], sent[1:])\n",
    "    ])\n",
    "    return b_freq, unigram(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "504c504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 5.380436055874065 Perplexity: 41.65552789238245\n"
     ]
    }
   ],
   "source": [
    "en_sent = sentences_bigram(en_text)\n",
    "en_model, en_uni = bigram(en_sent)\n",
    "en_total = en_model.total()\n",
    "\n",
    "en_entropy = -sum(\n",
    "    (count / en_total) * log(count / en_uni[w1], 2)\n",
    "    for (w1, _), count in en_model.items()\n",
    ")\n",
    "en_perplexity = 2 ** en_entropy\n",
    "\n",
    "print(f\"Entropy: {en_entropy} Perplexity: {en_perplexity}\")\n",
    "\n",
    "# Očekávaný výsledek: \n",
    "# en_entropy = 5.380173547272233\n",
    "# en_perplexity = 41.64794906297528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cdb9a6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 4.651925957848192 Perplexity: 25.140230250621457\n"
     ]
    }
   ],
   "source": [
    "cz_sent = sentences_bigram(cz_text)\n",
    "cz_model, cz_uni = bigram(cz_sent)\n",
    "cz_total = cz_model.total()\n",
    "\n",
    "cz_entropy = -sum(\n",
    "    (count / cz_total) * log(count / cz_uni[w1], 2)\n",
    "    for (w1, _), count in cz_model.items()\n",
    ")\n",
    "cz_perplexity = 2 ** cz_entropy\n",
    "\n",
    "print(f\"Entropy: {cz_entropy} Perplexity: {cz_perplexity}\")\n",
    "\n",
    "# Očekávaný výsledek: \n",
    "# cz_entropy = 4.624183207248371\n",
    "# cz_perplexity = 24.6614070105357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39e31073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SRILM příkazy\n",
    "#ngram-count -text TEXTEN_unigram.txt -order 2 -write czbigram.count -unk\n",
    "#ngram-count -gt1max 0 -gt2max 0 -read czbigram.count -order 2 -lm czbigram.lm\n",
    "#ngram -ppl TEXTEN_unigram.txt -order 2 -lm czbigram.lm >> czbigram.ppl\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
